# 解决Lasso问题的优化算法对比实验

## 一、实验目的

本实验实现了多种求解Lasso问题的优化算法，并通过对比实验分析不同算法的收敛性能。Lasso问题的目标函数为：

$$
f(x) = \frac{1}{2} \|Ax - b\|_2^2 + \lambda \|x\|_1
$$

第一项为loss项，第二项为L1正则化项。 

## 二、数据生成过程

数据生成由`create_lasso_problem`函数实现，具体流程如下：

1. **生成矩阵A**：
   - 随机生成形状为`(n_samples, n_features)`的矩阵，元素服从标准正态分布 $N(0,1)$ 
   - 按列归一化处理：计算每列的L2范数，将每列除以其范数（若范数为0则替换为1，避免除零错误），确保各特征列的尺度一致
2. **生成真实解x_true**：
   - 初始化为全零向量
   - 随机选择`sparsity * n_features`个索引作为非零元素位置（体现稀疏程度）
   - 非零元素服从正态分布 $N(0,1)$ ，缩放系数为1.0
3. **生成观测向量b**：
   - 基础值为`A @ x_true`，这是无噪声的理想观测
   - 加入噪声：添加服从 $N(0, 0.01^2)$ 的随机噪声，模拟真实观测中的误差

## 三、参数选择说明

### 核心参数设置

| 参数名          | 取值               | 说明                       |
| ------------ | ---------------- | ------------------------ |
| `n_trials`   | 10/50            | 实验重复次数，计算平均收敛曲线，减少随机误差影响 |
| `n_samples`  | 200/250/500/1000 | 样本数量                     |
| `n_features` | 500/250/200      | 特征数量，将其设置为高于样本数量可以模拟高维特性 |
| `sparsity`   | 0.1              | 真实解的稀疏度（非零元素占比）          |
| `lambda_val` | 0.1              | 正则化系数，控制稀疏性强度            |
| `max_iter`   | 300              | 最大迭代次数                   |
| `device`     | 优先`cuda`，否则`cpu` | 计算设备，优先使用GPU加速           |

### 算法特定参数

- **光滑梯度下降（Smoothed GD）**：
  
  `epsilon`：光滑化参数，取值1e-4、1e-6、1e-8，控制L1正则化的近似光滑程度。值越小近似越精确，但梯度可能越不稳定。

- **ADMM（交替方向乘子法）**：
  
  `rho`：惩罚参数，取值0.3、0.5、1.0、1.5、2.0，平衡约束满足程度与目标函数优化

- **梯度类算法（GD/Proximal GD/FISTA）**：
  
  学习率`lr`：默认取`1/L`（`L`为Lipschitz常数，由`A`的谱范数平方计算），保证收敛性的自适应学习率。但对于次梯度下降和光滑化梯度下降，为了防止曲线收敛到一个大精度就停止，实验中我们酌情减小学习率。

## 四、算法流程

### 1. 普通梯度下降（Ordinary GD）

- **原理**：基于次梯度下降，因L1正则化在0点不可导

- **步骤**：
  
  1. 计算loss项的梯度： $A^T (Ax - b)$ 
  
  2. 计算正则项次梯度：在次梯度集合中取0
  
  3. 总次梯度为两者之和。应用梯度裁剪：范数超过1e3时进行缩放防止梯度爆炸，以致算法不收敛
  
  4. 更新规则： $x = x - lr * 次梯度$

### 2. 近端梯度下降（Proximal GD）

- **原理**：通过近端算子处理非光滑项

- **步骤**：
  
  1. 计算光滑项梯度： $A^T (Ax - b)$，应用梯度裁剪
  
  2. 梯度下降步： $x_{temp} = x - lr * 梯度$
  
  3. 近端操作：对`x_temp`应用L1正则的软阈值算子`soft_threshold(x_temp, lr*λ)`计算

- 软阈值公式： $prox(x) = sign(x) * max(|x| - threshold, 0)$

### 3. 光滑梯度下降（Smoothed GD）

- **原理**：用光滑函数近似L1正则化，将非光滑问题转化为光滑问题

- **步骤**：
  
  1. 计算数据拟合项梯度： $A^T (Ax - b)$
  
  2. 计算光滑L1正则的梯度： $λx / \sqrt{(x² + ε)}$（ $ε$ 为光滑参数）
  
  3. 总梯度为两者之和，应用梯度裁剪
  
  4. 更新规则： $x = x - lr * 梯度$，并截断在`[-1e3, 1e3]`

### 4. ADMM（交替方向乘子法）

- **原理**：通过引入辅助变量将问题分解为可高效求解的子问题，利用对偶上升法优化

- **步骤**：
  
  1. **变量拆分**：引入辅助变量`z`，使目标等价于 $min \frac{1}{2}‖Ax - b‖² + λ‖z‖₁$ ,   $s.t. \quad  x = z$
  
  2. **x更新**：求解二次规划子问题，利用Cholesky分解高效计算 $x = (A^TA + ρI)^{-1} (A^Tb + ρ(z - u))$
  
  3. **z更新**：对`x + u`应用软阈值算子计算`z = soft_threshold(x + u, λ/ρ)`
  
  4. **对偶变量更新**： $u = u + x - z$

### 5. 坐标下降（Coordinate Descent）

- **原理**：每次更新一个变量（特征），固定其他变量，将高维问题降为一维子问题

- **步骤**：
  
  1. 预计算 $A^TA$,  $A^Tb$ 和各特征列的L2范数平方`norm_a_sq`
  
  2. 每次迭代随机打乱特征顺序，按顺序更新每个变量 $x_j$：

- 计算梯度分量： $c_j = a_j^T (b - A_{-j}x_{-j})$（排除第j个特征的贡献）

- 软阈值更新:
  
![软阈值更新规则](CodeCogsEqn.png)



### 6. FISTA（加速的近端梯度下降）

- **原理**：在近端梯度基础上引入动量加速，提高收敛速度

- **步骤**：
  
  1. 初始化动量参数`t = 1`，变量`x_prev = x`
  
  2. 计算加速点： $y = x + \frac{t-1}{t} (x - x_{prev})$
  
  3. 计算光滑项梯度并更新临时变量： $x_{temp} = y - lr * 梯度$
  
  4. 近端操作：`x_new = soft_threshold(x_temp, lr*λ)`
  
  5. 更新动量参数： $t_{new} = \frac{1 + \sqrt{1 + 4t²}}{2}$，`t = t_new`，`x_prev = x`，`x = x_new`

### 7. FISTA+Restart（带重启的加速近端梯度）

- **原理**：在FISTA基础上增加重启机制，当加速条件不满足时重置动量，避免收敛停滞

- **步骤**：与FISTA基本一致，增加重启条件：若 $\langle \nabla f(y),  x_{\text{new}} - x \rangle \geq 0$（加速方向无效），则重置`t = 1`并将加速点`y`重置为`x`

### 8. 基准方法：sklearn Lasso

- **原理**：调用scikit-learn的Lasso求解器作为最优解的基准

- **数量级适配处理**：因sklearn目标函数为 $\frac{1}{2n_{samples}}‖Ax - b‖² + α‖x‖₁$，将`α`设置为`λ / n_samples`以保持目标函数一致

## 五、实验结果展示

通过多次重复试验，绘制各算法的平均收敛曲线,  $f(x_k) - f^{\*}$ 的对数坐标， $f^{\*}$ 为基准方法的最优值，对比不同算法的收敛速度和稳定性。结果图中单次试验曲线是细浅色，平均曲线是粗深色。
### 1. 参数设置
本实验设计四类维度组合（样本量 $n$×特征数 $p$），覆盖不同数据稀疏性与维度特性： 
| 场景类型 | 维度配置 | 问题特性 |
|----------------|-------------|------------------------------|
| 低维场景1 | 1000×200 | 样本充足，特征维度低（ $n\gg p$） |
| 低维场景2 | 500×200 | 样本略减，特征维度低（ $n>p$） |
| 平衡场景 | 250×250 | 样本与特征数量匹配（ $n=p$） |
| 高维场景 | 200×500 | 样本稀缺，特征维度高（ $n\ll p$） |


算法变体参数：

- ADMM：设置惩罚参数 $\rho=0.3，0.5，1.0，1.5，2.0$共5种变体
- Smoothed GD：设置平滑参数 $\text{neighbor}=10^{-8}$、 $10^{-6}$、 $10^{-4}$三种变体

### 2.各场景收敛性能分析

#### 低维场景1（1000×200）

此场景下数据冗余度高，LASSO问题结构简单，算法收敛表现如下：

- **Coordinate Descent**：呈现断崖式下降，迭代前10步从 $10^0$量级骤降至 $10^{-4}$以下，是所有算法中初期收敛速度最快的。
  
- **FISTA vs FISTA-Restart**：标准FISTA前期下降速度接近CD，但在 $10^{-2}$精度后出现明显“锯齿震荡”；FISTA-Restart则完全消除震荡，以微小的速度代价换取了更平滑的收敛轨迹，基本与标准FISTA持平，但还是略快。
  
- **ADMM**：总体来说 $\rho=1.0，1.5$的表现最优，且这两个收敛速度比较近似；但在迭代前期（小于20步的时候）$\rho=0.3，0.5$收敛很快，20步之后速度逐渐慢于前者；而 $\rho=2.0$前期收敛速度与1.0/1.5持平，后期逐渐放缓，但放缓程度不如0.3/0.5明显。说明低维场景下**小惩罚参数**可适当加快原始--对偶变量的收敛速度，但如果需要收敛到较高的精度，又想收敛速度同时比较快，也需要适当加大惩罚参数。
  
- **Smoothed GD**：不同平滑参数对收敛速度影响不太大，在迭代的前50步展现出的性能十分相近。但我们观察到一些最终收敛精度的差异：在低维场景中，光滑程度越高，收敛最终精度反而要好一些，这可能是因为低维问题的最优解通常不呈现极端稀疏性，且样本矩阵条件数更优，此时$\varepsilon$增大带来的“梯度连续性提升、算法收敛稳定性增强”这一增益，会超过其“偏离原始L1正则”的近似损失；同时，更大的$\varepsilon$让光滑L1更接近适配低维密集解的L2特性，进一步助力算法稳定逼近最优值，而这一规律在高维稀疏场景中会因近似偏差过大而失效。
  
- **proximal GD**：近端梯度下降呈现十分优越的线性收敛，近乎一条直线，且收敛速度相较于次梯度和光滑后的梯度下降都有明显改善；但整体表现不如ADMM、FISTA和坐标下降。
  

#### 低维场景2（500×200）

样本量减少后，问题复杂度略有提升，但算法表现趋势与1000×200场景一致，差异体现在：

- Coordinate Descent的初期下降速度略有放缓，但仍保持绝对优势；
  
- ADMM的惩罚参数的敏感性增强：0.3与2.0的收敛差距增大，收敛到同样的精度需要的迭代步数更多，而0.3收敛后期的速度减慢是各种参数中最明显的；
  
- Smoothed GD各变体的收敛曲线差距缩小，速度和精度都很接近，说明样本量减少后，平滑参数的影响被弱化。
  
- proximal GD相较于次梯度和光滑梯度的优势仍然明显，但是收敛速度有所下降。
  

#### 平衡场景（250×250）

样本与特征维度匹配时，LASSO问题的病态性略有提升：

- Coordinate Descent的收敛速度比低场景略有减慢，但相较其他算法的优势仍然明显；
  
- FISTA-Restart仍然保持其收敛稳定性，收敛轨迹的平滑度远超标准FISTA，且收敛速度还是优于标准FISTA；
  
- ADMM各变体的收敛差距有所缩小，说明平衡场景下惩罚参数的敏感度有所降低，但整体收敛速度都有明显下降。
  
- Proximal GD与次梯度和光滑梯度的差距明显缩小，与ADMM、FISTA和坐标下降差距逐渐拉开，呈现出比较明显的恶化趋势。
  

#### 高维场景（200×500）

特征维度剧增导致问题病态性显著提升，算法表现分化加剧：

- **Coordinate Descent的优势仍然明显**：虽然所有算法在高维情况下收敛速度都有明显减缓,但是坐标下降仍是所有算法中表现最好的，且无震荡现象,一定程度上规避了高维梯度的维度灾难
  
- **FISTA的表现恶化**：标准FISTA仍有大幅锯齿震荡; FISTA-Restart的恶化十分明显, 在低维场景下FISTA-Restart的表现略优于我们选择的几个参数下的ADMM, 但在高维场景下ADMM反而相较与它体现出了优势: 更加稳定, 且对维度变化不如FISTA那么敏感
  
- **ADMM参数最优解漂移**：ADMM整体收敛速度都有恶化, 恶化最明显的是小惩罚参数, 但1.0和0.5相较于其他参数较优; 整体上ADMM系列与FISTA系列收敛速度是相近的
  
- **proximal GD性能崩塌**：近端梯度下降退化至和次梯度与光滑化梯度相近的收敛速度，恶化十分明显
  
- 总体来说, 高维情况下这些算法明显划分为三类: 最快的是Coordinate Descent; 其次是FISTA系列和ADMM系列; 最差的是proximal, 光滑化梯度和次梯度
  

### 3.算法性能的场景适配性

| 算法  | 低维场景 | 平衡场景 | 高维场景 | 最适配的场景 |
| --- | --- | --- | --- | --- |
| Coordinate Descent | 最好  | 最好  | 最好  | 全场景 |
| FISTA系列 | 次好  | 次好  | 次好  | 低维/平衡场景 |
| ADMM系列 | 次好  | 次好  | 次好  | 低维/平衡场景 |
| Proximal GD | 一般  | 一般  | 较差  | 低维/平衡场景 |
| Smoothed GD | 尚可  | 尚可  | 较差  | 低维场景 |
| sub GD | 尚可  | 尚可  | 较差  | 低维场景 |



附:

本文的SVG公式图片生成网站: https://editor.codecogs.com/
